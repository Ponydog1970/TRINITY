# TRINITY - API Integration Guide

Anleitung zur Integration von Cloud-APIs (OpenAI, Anthropic Claude) in die TRINITY App.

## √úbersicht

TRINITY kann in **3 Modi** arbeiten:

```
1. Nur Lokal (Standard)
   ‚úÖ 100% Privat
   ‚úÖ Offline
   ‚úÖ Keine Kosten
   ‚ö†Ô∏è Begrenzte Genauigkeit

2. Cloud-Unterst√ºtzt
   ‚úÖ Lokal f√ºr normale F√§lle
   ‚úÖ Cloud bei niedriger Confidence
   ‚ö†Ô∏è Kosten variieren
   ‚ö†Ô∏è Internet erforderlich

3. Cloud Bevorzugt
   ‚úÖ Beste Genauigkeit
   ‚ö†Ô∏è H√∂here Kosten
   ‚ö†Ô∏è Daten gehen an externe Server
   ‚ö†Ô∏è Internet erforderlich
```

---

## üöÄ Quick Start

### Schritt 1: API Keys besorgen

#### OpenAI (GPT-4 Vision)
```
1. Gehe zu: https://platform.openai.com/api-keys
2. Klicke "Create new secret key"
3. Kopiere: sk-proj-...
4. Speichere sicher!
```

**Kosten**: ~$0.01 pro Bildbeschreibung

#### Anthropic Claude (Claude 3.5 Sonnet Vision)
```
1. Gehe zu: https://console.anthropic.com/settings/keys
2. Klicke "Create Key"
3. Kopiere: sk-ant-api03-...
4. Speichere sicher!
```

**Kosten**: ~$0.008 pro Bildbeschreibung

### Schritt 2: API Keys in App eintragen

#### Via Code (f√ºr Entwicklung):
```swift
// In TrinityCoordinator.swift beim Init:
Configuration.shared.openAIKey = "sk-proj-..."
Configuration.shared.claudeKey = "sk-ant-..."
```

#### Via UI (f√ºr Endnutzer):
```
App √∂ffnen
‚Üí Settings
‚Üí Cloud-APIs
‚Üí API Keys konfigurieren
‚Üí Keys eingeben
‚Üí Speichern
```

### Schritt 3: Enhanced Perception Agent aktivieren

```swift
// In TrinityCoordinator.swift ersetzen:

// ALT:
self.perceptionAgent = try PerceptionAgent(
    embeddingGenerator: embeddingGenerator
)

// NEU:
self.perceptionAgent = try EnhancedPerceptionAgent(
    embeddingGenerator: embeddingGenerator,
    openAIKey: Configuration.shared.openAIKey,
    claudeKey: Configuration.shared.claudeKey,
    mode: .cloudEnhanced  // oder .localOnly, .cloudFirst
)
```

### Schritt 4: Testen!

```swift
// Build & Run
Cmd + R

// App √∂ffnet
‚Üí Start dr√ºcken
‚Üí Kamera auf Szene richten
‚Üí Warte auf Beschreibung

// Bei niedriger Confidence wird automatisch Cloud genutzt
```

---

## üìñ Detaillierte Integration

### 1. OpenAI Integration

#### Einfache Bildbeschreibung:

```swift
import Foundation

let openAI = OpenAIClient(apiKey: "sk-proj-...")

// Bild beschreiben
if let imageData = UIImage(named: "scene.jpg")?.jpegData(compressionQuality: 0.8) {
    let description = try await openAI.describeImage(imageData)
    print("GPT-4 Vision sagt: \(description)")
}
```

**Ausgabe:**
```
"Ich sehe einen Wohnraum mit einem Sofa links, einem Couchtisch mittig
und einer T√ºr rechts. Der Couchtisch ist etwa 1 Meter entfernt."
```

#### Navigation generieren:

```swift
let navText = try await openAI.generateNavigationDescription(
    objects: ["Tisch", "Stuhl", "T√ºr"],
    distances: [0.5, 1.2, 3.0],
    context: "Wohnzimmer, Nachmittag"
)
print(navText)
```

**Ausgabe:**
```
"Vorsicht: Tisch direkt vor Ihnen in 50cm Entfernung. Stuhl links in
1.2m. T√ºr geradeaus in 3m."
```

#### Text Embeddings:

```swift
let embedding = try await openAI.generateEmbedding(
    text: "Tisch vor mir im Wohnzimmer"
)
print("Embedding: \(embedding.count) Dimensionen")
// [0.23, -0.45, 0.67, ..., 0.12] (512 oder 1536 Dimensionen)
```

### 2. Anthropic Claude Integration

#### Szenenanalyse (strukturiert):

```swift
let claude = AnthropicClient(apiKey: "sk-ant-...")

let analysis = try await claude.analyzeScene(
    imageData,
    context: "Innenraum"
)

// Strukturierte Antwort:
print("Objekte:")
for obj in analysis.objects {
    print("- \(obj.name): \(obj.distance), \(obj.direction), Risiko: \(obj.risk)")
}

print("\nBeschreibung: \(analysis.sceneDescription)")
print("Navigation: \(analysis.navigationAdvice)")

if !analysis.warnings.isEmpty {
    print("\n‚ö†Ô∏è Warnungen:")
    for warning in analysis.warnings {
        print("- \(warning)")
    }
}
```

**Ausgabe:**
```
Objekte:
- Tisch: 0.5m, mittig, Risiko: hoch
- Stuhl: 1.2m, links, Risiko: mittel
- Wand: 3.0m, vorne, Risiko: niedrig

Beschreibung: Wohnraum mit M√∂beln, gut beleuchtet

Navigation: Bitte nach links ausweichen, Tisch direkt voraus

‚ö†Ô∏è Warnungen:
- Tisch sehr nah, Kollisionsgefahr
```

#### Kontextuelle Navigation:

```swift
let navText = try await claude.generateContextualNavigation(
    currentObservation: "Tisch vor mir",
    recentHistory: [
        "War gerade im Flur",
        "T√ºr nach links aufgemacht",
        "Jetzt im Raum"
    ],
    knownLocation: "Wohnzimmer"
)
```

**Ausgabe:**
```
"Sie sind ins Wohnzimmer gekommen. Der Couchtisch steht wie immer
mittig - gehen Sie links dran vorbei zum Sofa."
```

#### Memory-Konsolidierung:

```swift
let memories = [
    "Tisch im Wohnzimmer gesehen",
    "Wohnzimmer Tisch, mittags",
    "Couchtisch Wohnzimmer Nachmittag",
    "Stuhl neben Tisch im Wohnzimmer"
]

let consolidated = try await claude.consolidateMemories(memories: memories)
```

**Ausgabe:**
```
[
  "Couchtisch mittig im Wohnzimmer (mehrfach beobachtet)",
  "Stuhl neben Couchtisch"
]
```

### 3. Enhanced Perception Agent

#### Cloud-Enhanced Mode (Empfohlen):

```swift
let agent = try EnhancedPerceptionAgent(
    embeddingGenerator: embeddingGenerator,
    openAIKey: "sk-proj-...",
    claudeKey: "sk-ant-...",
    mode: .cloudEnhanced
)

let output = try await agent.process(input)

// Logik:
// 1. Versuche lokal (Core ML)
// 2. Wenn Confidence < 0.8 ‚Üí Nutze Cloud
// 3. Kombiniere Ergebnisse
```

**Vorteile:**
- ‚úÖ Meiste Zeit offline (privat + kostenlos)
- ‚úÖ Cloud nur bei Unsicherheit
- ‚úÖ Beste Balance Privacy/Genauigkeit

**Workflow:**
```
Frame kommt rein
‚Üí Vision Framework analysiert
‚Üí Confidence: 0.65 (niedrig!)
‚Üí Ruft Claude API
‚Üí Claude: Confidence 0.95
‚Üí Kombiniert Ergebnisse
‚Üí Finale Confidence: 0.95
```

#### Cloud-First Mode:

```swift
let agent = try EnhancedPerceptionAgent(
    embeddingGenerator: embeddingGenerator,
    claudeKey: "sk-ant-...",
    mode: .cloudFirst
)
```

**Vorteile:**
- ‚úÖ Maximale Genauigkeit
- ‚úÖ Detaillierte Beschreibungen
- ‚úÖ Strukturierte Ausgabe

**Nachteile:**
- ‚ö†Ô∏è H√∂here Kosten (~$10-30/Monat bei normaler Nutzung)
- ‚ö†Ô∏è Internet erforderlich
- ‚ö†Ô∏è Bilder gehen an externe Server

---

## üí∞ Kosten-Kalkulation

### OpenAI GPT-4 Vision
```
Preis: $0.01 pro Bild
Bei 100 Bildern/Tag: $1/Tag = ~$30/Monat
Bei 20 Bildern/Tag: $0.20/Tag = ~$6/Monat
```

### Anthropic Claude 3.5 Sonnet Vision
```
Preis: ~$0.008 pro Bild
Bei 100 Bildern/Tag: $0.80/Tag = ~$24/Monat
Bei 20 Bildern/Tag: $0.16/Tag = ~$5/Monat
```

### Empfohlene Nutzung:

**Cloud-Enhanced Mode** (Hybrid):
```
90% lokal (kostenlos)
10% Cloud (bei niedriger Confidence)

‚Üí Bei 100 Analysen/Tag:
   - 90 lokal: $0
   - 10 Cloud: $0.10
   ‚Üí ~$3/Monat
```

### Rate Limiting setzen:

```swift
// Max 50 Cloud-Calls pro Tag
Configuration.shared.maxCloudCallsPerDay = 50

// Pr√ºfen vor Call:
if !Configuration.shared.hasReachedCloudLimit() {
    let result = try await openAI.describeImage(imageData)
    Configuration.shared.incrementCloudCalls()
}
```

---

## üîê Sicherheit & Datenschutz

### API Keys sicher speichern:

‚úÖ **Gut:**
```swift
// Nutze Configuration (speichert in UserDefaults verschl√ºsselt)
Configuration.shared.openAIKey = "sk-..."
```

‚ùå **NIEMALS:**
```swift
// Nicht im Code hardcoden!
let key = "sk-proj-abc123..."  // ‚ùå Landet in Git!

// Nicht in Klartext loggen!
print("API Key: \(key)")  // ‚ùå Landet in Logs!
```

### Environment Variables (Entwicklung):

```bash
# .env Datei (NICHT committen!)
OPENAI_API_KEY=sk-proj-...
CLAUDE_API_KEY=sk-ant-...

# .gitignore
.env
*.env
```

```swift
// In Code laden:
Configuration.shared.loadFromEnvironment()
```

### User Consent:

```swift
// Immer User fragen vor Cloud-Nutzung!
if !Configuration.shared.allowCloudProcessing {
    // Zeige Alert
    "Diese Funktion sendet Bilder an externe Server. Erlauben?"

    if userAllows {
        Configuration.shared.allowCloudProcessing = true
    }
}
```

---

## üß™ Testing

### Mock API f√ºr Testing:

```swift
class MockOpenAIClient: OpenAIClient {
    override func describeImage(_ imageData: Data, prompt: String) async throws -> String {
        return "Test-Beschreibung: Tisch vor Ihnen"
    }
}

// In Tests verwenden:
let agent = try EnhancedPerceptionAgent(
    embeddingGenerator: embeddingGenerator,
    mode: .localOnly  // Keine echten API Calls
)
```

### Rate Limit Testing:

```swift
func testRateLimit() async throws {
    Configuration.shared.maxCloudCallsPerDay = 10
    Configuration.shared.resetDailyCloudCalls()

    for i in 1...15 {
        if !Configuration.shared.hasReachedCloudLimit() {
            Configuration.shared.incrementCloudCalls()
            print("Call #\(i) erfolgreich")
        } else {
            print("Rate Limit erreicht bei Call #\(i)")
            break
        }
    }
}
```

---

## üìä Monitoring & Analytics

### Kosten-Tracking:

```swift
// T√§glich Kosten tracken
extension Configuration {
    func logCloudUsage() {
        let cost = Double(cloudCallsToday) * 0.01  // $0.01 pro Call
        print("Heute: \(cloudCallsToday) Calls, $\(cost)")
    }
}

// Monatliche Sch√§tzung
let monthlyCost = Configuration.shared.estimatedMonthlyCost()
print("Gesch√§tzt diesen Monat: $\(monthlyCost)")
```

### Performance-Vergleich:

```swift
// Messe Latenz
let start = Date()

// Lokal
let localOutput = try await localAgent.process(input)
let localTime = Date().timeIntervalSince(start)

// Cloud
let cloudStart = Date()
let cloudOutput = try await enhancedAgent.process(input)
let cloudTime = Date().timeIntervalSince(cloudStart)

print("""
Lokal: \(Int(localTime * 1000))ms, Confidence: \(localOutput.confidence)
Cloud: \(Int(cloudTime * 1000))ms, Confidence: \(cloudOutput.confidence)
""")
```

---

## üéØ Best Practices

### 1. Hybrid Approach (Empfohlen)

```swift
// Nutze Lokal f√ºr normale F√§lle
// Cloud nur bei:
// - Niedriger Confidence
// - Komplexen Szenen
// - User-Request

if localConfidence > 0.8 || !networkAvailable {
    return localResult
} else {
    return cloudResult
}
```

### 2. Offline-F√§higkeit

```swift
// Immer Fallback auf Lokal
do {
    if networkAvailable && allowCloud {
        return try await cloudProcess()
    }
} catch {
    print("Cloud failed, fallback to local")
}
return try await localProcess()
```

### 3. User-Kontrolle

```swift
// User entscheidet:
// - Cloud ja/nein
// - Rate Limits
// - Kosten-Budget
// - API Anbieter (OpenAI vs Claude)

if Configuration.shared.canUseCloudAPIs() {
    // User hat explizit erlaubt
}
```

---

## üîß Troubleshooting

### Problem: "Invalid API Key"

```swift
// Pr√ºfe Key-Format:
if !Configuration.shared.hasValidOpenAIKey() {
    print("OpenAI Key ung√ºltig: muss mit 'sk-' beginnen")
}

if !Configuration.shared.hasValidClaudeKey() {
    print("Claude Key ung√ºltig: muss mit 'sk-ant-' beginnen")
}
```

### Problem: "Rate Limit Exceeded"

```swift
// Option 1: T√§glich zur√ºcksetzen
Configuration.shared.resetDailyCloudCalls()

// Option 2: Limit erh√∂hen
Configuration.shared.maxCloudCallsPerDay = 100

// Option 3: Upgrade API Plan
// ‚Üí OpenAI Tier erh√∂hen
// ‚Üí Claude Credits kaufen
```

### Problem: "Network Error"

```swift
// Fallback auf Lokal
do {
    return try await cloudProcess()
} catch {
    print("Network Error: \(error)")
    print("Fallback to local processing")
    return try await localProcess()
}
```

---

## üì± UI Integration

### Settings mit Cloud-Konfiguration:

```swift
// Verwende EnhancedSettingsView statt SettingsView

.sheet(isPresented: $showSettings) {
    EnhancedSettingsView()  // Neue Version mit API Config
        .environmentObject(coordinator)
}
```

**Features:**
- ‚úÖ API Key Eingabe (maskiert)
- ‚úÖ Mode-Auswahl (Lokal/Hybrid/Cloud)
- ‚úÖ Rate Limit Konfiguration
- ‚úÖ Kosten-Sch√§tzung
- ‚úÖ Privacy-Warnungen

---

## üöÄ Deployment

### Production Checklist:

```
‚úÖ API Keys aus Code entfernen
‚úÖ .env in .gitignore
‚úÖ User Consent UI implementiert
‚úÖ Rate Limiting aktiviert
‚úÖ Fallback auf Lokal funktioniert
‚úÖ Kosten-Warnung bei Cloud-Nutzung
‚úÖ Privacy Policy aktualisiert
```

### App Store Submission:

**Info.plist:**
```xml
<key>NSAppTransportSecurity</key>
<dict>
    <key>NSAllowsArbitraryLoads</key>
    <false/>
</dict>
```

**Privacy Manifest:**
```
Data Sent to Third Parties:
- OpenAI: Images for object recognition (optional)
- Anthropic: Images for scene analysis (optional)

User Control: Yes, can be disabled in Settings
```

---

## üìö Weiterf√ºhrende Ressourcen

- [OpenAI API Docs](https://platform.openai.com/docs)
- [Anthropic Claude Docs](https://docs.anthropic.com)
- [Apple Privacy Guidelines](https://developer.apple.com/privacy)
- [iOS Security Best Practices](https://developer.apple.com/security)

---

**Status**: ‚úÖ API Integration vollst√§ndig implementiert

**N√§chste Schritte**: API Keys besorgen & testen!
