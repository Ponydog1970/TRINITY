# TRINITY Vision Aid - iOS App

Eine fortschrittliche Navigationshilfe-App fÃ¼r Sehbehinderte, die Apple Intelligence, LiDAR und fortgeschrittene KI-Technologie nutzt.

## Ãœberblick

TRINITY kombiniert:
- **Apple Intelligence**: On-device ML fÃ¼r Echtzeit-Szenenerkennung
- **LiDAR Scanner**: PrÃ¤zise rÃ¤umliche Tiefenerfassung (iPhone 17 Pro)
- **RAG/MAS System**: Intelligente Kontextverwaltung mit 3-Schicht-GedÃ¤chtnis
- **Lokale Verarbeitung**: Alle Daten bleiben auf dem GerÃ¤t
- **VoiceOver optimiert**: VollstÃ¤ndige Barrierefreiheit

## Systemanforderungen

- **iOS**: 17.0 oder hÃ¶her
- **GerÃ¤t**: iPhone 17 Pro (oder neuer mit LiDAR)
- **Speicher**: Mindestens 500 MB frei
- **Berechtigungen**: Kamera, Standort, AR

## Architektur-Komponenten

### 1. Memory System (3-Layer)
```
Working Memory    â†’ KurzzeitgedÃ¤chtnis (aktuelle Szene)
Episodic Memory   â†’ Besuchte Orte, zeitliche Ereignisse
Semantic Memory   â†’ Langzeit-Muster, gelernte Konzepte
```

### 2. Multi-Agent System
- **Perception Agent**: Verarbeitet Kamera + LiDAR Daten
- **Navigation Agent**: Hinderniserkennung + WegfÃ¼hrung
- **Context Agent**: Verwaltet Kontext Ã¼ber Memory-Schichten
- **Communication Agent**: Generiert natÃ¼rliche Sprachausgabe

### 3. RAG Pipeline
```
Sensor Input â†’ Embedding â†’ Vector Search â†’ Context â†’ Audio Output
```

### 4. Lokale KI
- **Embedding-Generierung**: Core ML Models
- **Vektor-Datenbank**: HNSW fÃ¼r schnelle Suche
- **Deduplizierung**: Verhindert redundante Informationen

## Projekt-Struktur

```
TrinityApp/
â”œâ”€â”€ Sources/
â”‚   â”œâ”€â”€ App/
â”‚   â”‚   â”œâ”€â”€ TrinityApp.swift           # Main App Entry
â”‚   â”‚   â””â”€â”€ TrinityCoordinator.swift   # System Coordinator
â”‚   â”œâ”€â”€ Agents/
â”‚   â”‚   â”œâ”€â”€ Agent.swift                # Base Agent Protocol
â”‚   â”‚   â”œâ”€â”€ PerceptionAgent.swift      # Vision + LiDAR Processing
â”‚   â”‚   â”œâ”€â”€ NavigationAgent.swift      # Navigation + Obstacles
â”‚   â”‚   â”œâ”€â”€ ContextAgent.swift         # Context Management
â”‚   â”‚   â””â”€â”€ CommunicationAgent.swift   # Speech + Feedback
â”‚   â”œâ”€â”€ Memory/
â”‚   â”‚   â”œâ”€â”€ MemoryManager.swift        # 3-Layer Memory Manager
â”‚   â”‚   â””â”€â”€ DeduplicationEngine.swift  # Duplicate Detection
â”‚   â”œâ”€â”€ VectorDB/
â”‚   â”‚   â””â”€â”€ VectorDatabase.swift       # Local Vector Storage
â”‚   â”œâ”€â”€ Sensors/
â”‚   â”‚   â””â”€â”€ SensorManager.swift        # Camera + LiDAR + Location
â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â””â”€â”€ MemoryLayer.swift          # Data Models
â”‚   â”œâ”€â”€ Utils/
â”‚   â”‚   â””â”€â”€ EmbeddingGenerator.swift   # Core ML Embeddings
â”‚   â””â”€â”€ UI/
â”‚       â””â”€â”€ MainView.swift             # SwiftUI Interface
â”œâ”€â”€ Tests/
â”œâ”€â”€ Resources/
â””â”€â”€ Info.plist
```

## Setup in Xcode

### 1. Xcode Projekt erstellen

1. Ã–ffne Xcode
2. **File â†’ New â†’ Project**
3. WÃ¤hle **iOS â†’ App**
4. Projekt-Einstellungen:
   - **Product Name**: TRINITY
   - **Team**: Dein Apple Developer Team
   - **Organization Identifier**: com.trinity
   - **Bundle Identifier**: com.trinity.visionaid
   - **Interface**: SwiftUI
   - **Language**: Swift
   - **Minimum iOS**: 17.0

### 2. Dateien importieren

1. Kopiere alle `.swift` Dateien aus `Sources/` in das Xcode Projekt
2. Organisiere Dateien in Gruppen entsprechend der Struktur
3. FÃ¼ge `Info.plist` hinzu

### 3. Frameworks hinzufÃ¼gen

In **Target â†’ Build Phases â†’ Link Binary With Libraries**:
- `ARKit.framework`
- `AVFoundation.framework`
- `CoreML.framework`
- `Vision.framework`
- `CoreLocation.framework`
- `SwiftUI.framework`
- `CloudKit.framework`
- `NaturalLanguage.framework`

### 4. Capabilities aktivieren

In **Target â†’ Signing & Capabilities**:
- âœ… **iCloud** (CloudKit)
- âœ… **Background Modes** (Location updates, Audio)
- âœ… **Maps**

### 5. Info.plist konfigurieren

Kopiere die Berechtigungen aus `Info.plist`:
- Camera Usage Description
- Location Usage Description
- ARKit Usage Description

### 6. Core ML Models hinzufÃ¼gen

Core ML Models mÃ¼ssen separat trainiert oder heruntergeladen werden:

1. **Vision Model** (Objekterkennung):
   - Download: MobileNetV3 oder YOLOv8 Core ML
   - Drag & Drop in Xcode

2. **Text Embedding Model** (optional):
   - Nutzt native `NLEmbedding`

### 7. Build & Run

1. WÃ¤hle iPhone 17 Pro Simulator oder physisches GerÃ¤t
2. **Cmd + R** zum Bauen und AusfÃ¼hren
3. Erlaube alle Berechtigungen (Kamera, Standort, AR)

## Verwendung

### Erste Schritte

1. **App starten**
2. **Berechtigungen erteilen**: Kamera, Standort, AR
3. **"Start" drÃ¼cken**: TRINITY aktivieren
4. **GerÃ¤t schwenken**: Umgebung erfassen
5. **ZuhÃ¶ren**: Audio-Beschreibungen und Navigation

### Hauptfunktionen

#### 1. Automatische Szenenbeschreibung
- TRINITY beschreibt kontinuierlich die Umgebung
- Erkennt Objekte, Hindernisse, RÃ¤ume
- Passt VerbositÃ¤t an (Minimal/Medium/Detailed)

#### 2. Navigation
- Hinderniserkennung mit LiDAR
- Audio-Warnungen bei Gefahren
- RoutenvorschlÃ¤ge basierend auf Historie

#### 3. Kontext-Bewusstsein
- "Ich war schon hier"-Erkennung
- HÃ¤ufig besuchte Orte merken
- Zeitliche Muster lernen

#### 4. Sprachausgabe
- NatÃ¼rliche deutsche Sprachausgabe
- Priorisierung: Sicherheitswarnungen zuerst
- Haptisches Feedback bei Hindernissen

### Gesten & Steuerung

- **Einmal tippen**: Szene beschreiben
- **Zweimal tippen**: Letzte Nachricht wiederholen
- **Dreimal tippen**: Navigation starten
- **Lange drÃ¼cken**: Einstellungen Ã¶ffnen

### Settings

- **Verbosity**: Minimal/Medium/Detailed
- **Memory Management**: Konsolidieren/LÃ¶schen
- **iCloud Sync**: Export/Import

## Barrierefreiheit

### VoiceOver Optimierungen

- Alle UI-Elemente haben **accessibilityLabel**
- GroÃŸe Touch-Targets (min. 44x44 pt)
- Hoher Kontrast (WCAG AAA)
- Keine zeitkritischen Interaktionen

### Haptic Feedback

- **Leicht**: Weit entferntes Hindernis
- **Mittel**: Nahe Objekte
- **Stark**: Kritische Warnung
- **Muster**: Navigationshinweise (Links/Rechts)

### Audio Feedback

- **3D Audio**: RÃ¤umliche Positionierung von Hindernissen
- **Beep-Frequenz**: Distanz-Kodierung
- **LautstÃ¤rke**: PrioritÃ¤ts-Kodierung

## Datenschutz & Sicherheit

### Lokale Verarbeitung
- **Alle Embeddings lokal**: Keine Cloud-API
- **Alle Daten on-device**: Keine Server-Kommunikation
- **Core ML on-device**: Apple Neural Engine

### iCloud Sync (optional)
- **Ende-zu-Ende verschlÃ¼sselt**: CloudKit
- **User-kontrolliert**: Opt-in
- **KonfliktauflÃ¶sung**: Neueste Daten gewinnen

### Berechtigungen
- **Kamera**: Nur wÃ¤hrend App-Nutzung
- **Standort**: When In Use
- **ARKit**: Erforderlich fÃ¼r LiDAR
- **Keine Telemetrie**: Kein Tracking

## Performance

### Optimierungen

- **Batch Processing**: Mehrere Embeddings parallel
- **Adaptive Quality**: Reduzierte AuflÃ¶sung bei niedrigem Akku
- **Memory Management**: LRU Cache fÃ¼r hÃ¤ufige Abfragen
- **Neural Engine**: Maximale GPU/ANE Nutzung

### Benchmark (iPhone 17 Pro)

- **Embedding-Generierung**: ~50ms
- **Vector Search**: ~10ms (10k Vektoren)
- **End-to-End Latenz**: ~200ms (Sensor â†’ Audio)
- **Akku-Verbrauch**: ~15%/Stunde bei kontinuierlicher Nutzung

## Testing

### Unit Tests
```bash
# In Xcode: Cmd + U
```

### Integration Tests
- Sensor-Mock-Daten verwenden
- Memory-Persistence testen
- Agent-Koordination testen

### Accessibility Tests
- VoiceOver Compatibility
- Dynamic Type Support
- High Contrast Mode

### Real-World Testing
- Mit Sehbehinderten testen
- Indoor + Outdoor Szenarien
- Verschiedene LichtverhÃ¤ltnisse

## Roadmap

### v1.0 (Aktuell)
- âœ… Basic LiDAR Integration
- âœ… 3-Layer Memory
- âœ… Multi-Agent System
- âœ… VoiceOver Support
- âœ… Lokale Embeddings

### v1.1 (Geplant)
- [ ] Custom Core ML Models (Fine-tuning)
- [ ] Offline-Karten Integration
- [ ] Favoriten-Orte
- [ ] Sprachbefehle
- [ ] Apple Watch Companion

### v1.2 (Zukunft)
- [ ] Objektverfolgung Ã¼ber Zeit
- [ ] Gesichtserkennung (Bekannte Personen)
- [ ] Indoor-Positionierung
- [ ] Multi-Device Sync (iPad, Mac)

## Lizenz

Proprietary - Alle Rechte vorbehalten

## Support

Bei Fragen oder Problemen:
- **GitHub Issues**: [Repository Issues]
- **Email**: support@trinity-visionaid.com
- **Docs**: [Dokumentation]

## Credits

- **Entwickelt fÃ¼r**: Menschen mit Sehbehinderung
- **Technologie**: Apple ARKit, Core ML, SwiftUI
- **Inspiration**: Barrierefreie Navigation fÃ¼r alle

---

**TRINITY** - Sehen mit kÃ¼nstlicher Intelligenz ğŸ‘ï¸ğŸ¤–
